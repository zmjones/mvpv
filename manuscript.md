Although usually studied separately it many manifestations of political violence may be related to a similar set of structural characteristics of the countries in which said violence occurs (e.g., consider the omnipresence of measures of wealth, government structure, etc. in empirical models of interstate and intrastate conflict of various sorts). Due to our limited knowledge of the functional form of the joint relationship between the set of structural features and political violence, as measured, we make use of a nonparametric multivariate regression that can detect nonlinear, discontinuous, interactive relationships while not overfitting the data. Specifically we make use of an ensemble of multivariate randomized conditional inference trees [@hothorn2006unbiased] which are similar to a random forest [@breiman2001random], which itself is a randomized version of bagged classification or regression trees (CART) [@breiman1999classification; @breiman1996bagging].

CART constructs a peicewise constant approxmation to the regression function by iteratively partitioning the response variable using the covariates and predicting using a constant function of the response variable in the resulting partitions. CART, bagging, and random forests are described in greater detail by @jones2015rfss and @friedman2001elements. A brief recapitulation follows. Suppose for example that we wish to predict a discrete outcome from several covariates. Beginning at the root, i.e., initial, node, which contains all of the sample data, a classification tree would first estimate the univariate relationship between each covariate and the outcome variable. The variable with the strongest relationship is selected. Then for every observed value that the selected variable takes, the expected reduction in the loss is computed, that is, the difference between the loss computed on the data at the current node (in this case the root node) and the sum of the loss if the data were to be partitioned using this value, and the loss computed for each partition. This process is repeated until a stopping criteria is met, resulting in a set of recursive partitions of the data. In the terminal nodes (i.e., an exhaustive, dijoint set of partitions, wherein no further splitting occurred, that is, nodes at the maximal depth of the tree) the prediction is a constant function of the data in said node. For classification the modal category is usually used. For regression the choice is often the mean or median.

Ensembles of CART are used primarily to reduce the variance of predictions. Bagging (bootstrap aggregating) works by randomizing the data used to grow each tree (e.g. by drawing observations with replacement from the original data), and then averaging across each tree's predictions [@breiman1996bagging]. This has the advantage of making the trees' predictions less correlated and thus reducing sampling variance of the predictions. It also provides a built-in estimator of the expected error on new data (generalization error) since each tree can (optionally) make predictions only for the data that were not used to grow said data (the out-of-bag, or OOB data). Random selection of the available variables for splitting at each node results in random forests, which result in a further reduction in variance by further reducing correlation between trees in the ensemble. Random forests have been empirically successful in comparison to other modern machine learning methods and are less prone to overfitting than CART or bagged CART (i.e., reducing bias at the cost of increased variance such that the overall expected error increases) [@fernandez2014we].

CART (and thus ensembles thereof) can be extended to multivariate responses by stacking the loss function for each response such that a split occurs if and only if the loss across all responses is decreased enough to justify a split [@de2002multivariate; @segal2011multivariate; @hothorn2006unbiased]. CART as developed in @breiman2001random exhibit splitting behavior biased towards covariates with many values, e.g., continuous covariates are preferred to discrete covariates even in the case where (by construction) none have any relationship with the response. @hothorn2006unbiased develop conditional inference trees which avoid this problem by using a permutation statistic to measure the relationship between each covariate and the response, and then computing a $p$-value for this statistic, which is scale-invariant.

Although the above-mentioned algorithm generates predictions for each response as a function of the covariates in a way that (heuristically) minimizes the expected error on new data from the same historical data generating process, the learned function is not directly interpretable. While CART are themselves interpretable with a univariate response, they are less so with a multivariate response. Additionally, the fact that each tree in the ensemble is fit using data that are a synthetic (in this case, a subsample of $.632$ of the data) and the available (for splitting) covariates at each node are a random subset of the covariates in the full data, makes the ensemble uninterpretable. This obstacle is overcome by marginalizing the learned function, specifically by averaging over the features that are not of interest. Suppose we are given data $(\mathbf{Y}, f(\mathbf{X})) \sim \mathbb{P}$ where $\mathbb{P} = \mathcal{X} \times \mathcal{Y}$ is a probability distribution over the sample space for $X$ and $Y$ resulting in realizations $\mathbf{X}$ and $\mathbf{Y}$. We can apply an estimation method to $(\mathbf{X}, \mathbf{Y})$ to learn $f$, usually the conditional expectation function, $\mathbb{E}(Y | X = \mathbf{X})$, resulting in $\hat{f}$. To interpret the relationship between a subset of $\mathbf{X}$, say $\mathbf{X}_S$ (a subset of the columns of $\mathbf{X}$ represented by a set of points) and the learned function $\hat{f}$ we need to marginalize $\hat{f}$ over the distribution of $\mathbf{X}_{-S}$. Since we have not assumed a full probability model this must be done empirically. In particular $\hat{f}_{\mathbf{X}_s}(\mathbf{X}) = \frac{1}{N} \sum_{i = 1}^N \hat{f}(\mathbf{X}_S, \mathbf{X}^{(i)}_{-S})$ The partial dependence algorithm was first proposed by @friedman2001greedy, and is further described in @friedman2001elements, @jones2015rfss, and extended in @goldstein2015peeking. Implementations of the algorithm which work with any univariate response variable given a learned function are available in @goldstein2015package and @bischl2015package. Although typically applied to learned functions which map a multivariate set of covariates to a univariate response, its application to a function mapping multivariate covariates to a multivariate response requires no modification.
